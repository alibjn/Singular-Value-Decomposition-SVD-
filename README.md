Singular-Value-Decomposition-SVD-

The singular value decomposition (SVD) is among the most important matrix factorizations of the computational era, providing a foundation for nearly all of the data methods in this book. We will use the SVD to obtain low-rank approximations to matrices and to perform pseudo-inverses of non-square matrices to find the solution of a system of equations. Another important use of the SVD is as the underlying algorithm of principal component analysis (PCA), where high-dimensional data is decomposed into its most statistically descriptive factors. SVD/PCA has been applied to a wide variety of problems in science and engineering.

In many domains, complex systems will generate data that is naturally arranged in large matrices, or more generally in arrays. For example, a time-series of data from an experiment or a simulation may be arranged in a matrix with each column containing all of the measurements at a given time. If the data at each instant in time is multi-dimensional, as in a high-resolution simulation of the weather in three spatial dimensions, it is possible to reshape or flatten this data into a high-dimensional column vector, forming the columns of a large matrix. Similarly, the pixel values in a grayscale image may be stored in a matrix, or these images may be reshaped into large column vectors in a matrix to represent the frames of a movie. Remarkably, the data generated by these systems are typically low rank, meaning that there are a few dominant patterns that explain the high-dimensional data. The SVD is a numerically robust and efficient method of extracting these patterns from data.


(SVD) is a linear aljebra method that decomposes a matrix into tree resultant matrices to reduce information redundancy and noise,

A = U . Sigma . V^T

![1_4Vpi3CFxjLsyJ2zZsZfcCw](https://user-images.githubusercontent.com/66758522/95274209-14355980-083d-11eb-92c8-125c61aefac5.png)


Where  


A : The original Matrix , is the real m x n matrix that we wish to decompose.

U : left orthogonal matrix ; holds important ,nonredundant information about observations , is an m x m matrix.

Sigma : diagonal matrix ; contains all of the information about decomposition processes performed during the compression ,(often represented by the uppercase Greek letter Sigma) is an m x n diagonal matrix.

V^T : right orthogonal matrix ;holds important ,nonredundant information on features.

![Singular-value-decomposition-of-A-a-full-rank-r-b-rank-k-approximation](https://user-images.githubusercontent.com/66758522/95275070-70997880-083f-11eb-96b2-5a0010a732a0.png)


The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning. SVD can also be used in least squares linear regression, image compression, and denoising data.

-the advantages of using SVD .
                 
                    -SVD provides optimal representation of image by packing most of the information in few coefficients for a given image.
                    -Data Reduction
                    -Data-Driven -generalization of Fourier Transform (FFT) . 
                     "" The FFT is the most useful  transformations and all of the mathimatics based on sine and cosine expansions , and this technique is the last Generation  of                         computational Science, and engineering   was to use these mathimatical modes 'FFT' to map a system od interest into some new coordinate system where would                         become simple.""
                    - 'Tailord'    to specific problem .
                    - Matrix decomposition
                    - Simple \ Interpretable Linear Algebra
                    - good for "Big Data"
                    - extract  most important features (dominant)
                    - Strongly correlated 

Here the professor Steve Brunton explains it beautifully ==>    https://www.youtube.com/watch?v=gXbThCXjZFM&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv
